{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import validators\n",
    "from langchain_community.document_loaders import YoutubeLoader, UnstructuredURLLoader, WebBaseLoader\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:66.0) Gecko/20100101 Firefox/66.0\",\n",
    "               \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "               \"Accept-Language\": \"en-US,en;q=0.5\", \"Accept-Encoding\": \"gzip, deflate\", \"DNT\": \"1\",\n",
    "               \"Connection\": \"close\", \"Upgrade-Insecure-Requests\": \"1\"}\n",
    "\n",
    "\n",
    "def get_document(url):\n",
    "    if validators.url(url):\n",
    "        if 'youtube' in url or 'youtu.be' in url :\n",
    "            loader = YoutubeLoader.from_youtube_url(url)\n",
    "        else : \n",
    "            #loader = UnstructuredURLLoader(urls=[url], ssl_verify= False, headers = headers)\n",
    "            loader = WebBaseLoader(web_path=url)\n",
    "        return loader.load()\n",
    "    else :\n",
    "        \n",
    "\n",
    "        print('Invalid URL')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_doc=get_document(\"https://www.youtube.com/watch?v=rUUvo3lNPTA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'rUUvo3lNPTA'}, page_content=\"[Music] hi welcome to another video so metrol has recently launched their new model metrol large 2 this came one day after the Llama 3 45b launch which is why I wasn't able to cover it but I thought it also needs to be covered so today I'll be covering it as the name suggests metrol large 2 is a new version of their mistra large model it is a 123 billion parameter model that's why it's named large it has about a 128k context window and supports dozens of languages along with 80 plus coding languages so that's also cool they say that mraw large 2 sets a new frontier in terms of performance cost of serving on evaluation metrics they also say that it is great in coding and on par with leading models like GPT 4 Claude 3 Opus and llama 3 45b you can see the chart for the same thing that they have also shared here so it's good to see that while being smaller it can be on par with bigger models like 405b or others one thing that I also like is that they have trained it to acknowledge when it cannot find Solutions or does not have sufficient information to provide a confident answer this basically means that it can say no I have seen a bunch of models that try to answer anything you ask and say yes to everything even if it's not possible or does not exist so that's also quite cool now let's look at the benchmarks I don't know why they have shared the benchmark like this I mean there's no exact number for the benchmarks you can only look at this chart and guess the number which is not cool at all I don't know what they achieve from this but they have shared it like this and have not shared the exact numbers for the benchmarks they have only shared the exact numbers for human evil anyway in human evil they say that it comes near GPT 40 and beats Claude 3.5 Sonet now if you heard that right you'll think how's that possible because Claude 3.5 Sonet had already beaten GPT 40 so how's the GPT 40 above the Claude 3.5 Sonet model in this chart and beats that well it's because they have put the incorrect numbers in their chart here's the number that Claude had shared in their post which is 92% but if you look at the chart here you'll see that they have put claw in about the 85 to 87% range which obviously doesn't make sense they have obviously done this to show that their model is superior but this just breaks my trust even more on these benchmarks and the craziest part is that they have almost done it everywhere and I think that's also the reason why they haven't shared the exact numbers so I won't even look at the benchmarks because it's all fabricated anyway the model's license is also not good it's under their custom license which only allows you to use it for free for research purposes or non-commercial purposes so that's also really bad the model is available to try on their own platform so let's try it out from there and see if it's good or not these are the nine questions that I'm going to try it against so let's get started the first question is what is the number that rhymes with the word we use to describe a tall plant the answer should be three let's see what it answers so it answers correctly let's give this a pass next one is I have two apples then I buy two more I bake a pie with two of the apples after eating half of the pie how many apples do I have left the answer should be two let's send it and check here's the answer and it looks fine so let's keep it a pass the next question is Sally is a girl she has three brothers each of her brothers has the same two sisters how many sisters does Sally have the answer should be one let's send it here's the answer and it's correct let's keep this a pass as well the next question is if a regular hexagon has a short diagonal of 64 what is its long diagonal the answer should be 73.9 here's the answer and it doesn't answer this question correctly so this one's a fail the next question is create an HTML page with a button that explodes confetti when you click it you can use CSS and JS as well let's send it here's the answer let's preview it so this technically works but I wouldn't give this a pass since they compare it against GPT 40 and Claude and then it produces this so this one's a fail the next question is write a python function that prints the next 20 leap years reply with only the function let's send it here's the code let's copy and run it so this works fine let's keep it a pass the next question is generate the SVG code for a butterfly let's see if it can make it here's the code let's preview it it doesn't look anything like a butterfly so that's a fail the next question is write an HTML page this HTML page is a landing page for an aias company they prefer a modern and minimal looking interface with animations let's send it and check okay here's the code let's preview it so this looks fine nothing too much and it did what was asked for so this one's a pass the next question is write a game of life in Python that works on the terminal let's see if it can give the correct code here's the code let's copy it and run it okay so this works so this one's also a pass now here's the final chart so this model looks fine although I wouldn't say that it's very good I mean it wasn't able to answer all of them correctly and especially when I see a model not passing this math question I know that it's not on par with Claude or GPT llama 345b was also able to answer that question but it couldn't and the generated code for confetti was also not good so it's not very good but it's fine I wouldn't use it a lot since deep seek is much cheaper than this in API pricing and is better than its quality but overall it can be cool for some things anyway let me know your thoughts in the comments if you liked this video consider donating to my Channel Through the super thanks option below also give this video a thumbs up and subscribe to my channel I'll see you in the next video till then bye [Music]\")]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youtube_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_doc = get_document(\"https://www.vedantu.com/english/apj-abdul-kalam-speech\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def document_splitter(document , chunk_size = 1000, chunk_overlap = 200):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size = chunk_size, chunk_overlap = chunk_overlap)\n",
    "    return splitter.split_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = document_splitter(web_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "groq_api = os.getenv(\"GROQ_API_KEY\")\n",
    "llm = ChatGroq(groq_api_key=groq_api, model=\"Gemma-7b-It\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Metrol Large 2: New Large Language Model from Metrol**\\n\\nMetrol has released their new large language model, Metrol Large 2, boasting 123 billion parameters. It claims to outperform GPT 4, Claude 3.5 Sonet, and other leading models.\\n\\n**Features:**\\n\\n- 128k context window\\n- Support for dozens of languages and 80+ coding languages\\n- Ability to acknowledge limitations and provide \"no\" answers when insufficient information is available.\\n\\n**Benchmarks:**\\n\\n- Benchmark results are presented as a chart without exact numbers, leading to suspicion of data manipulation.\\n- Claimed performance figures for Claude 3.5 Sonet and GPT 40 appear inconsistent with published data.\\n\\n**Evaluation:**\\n\\n- The model performs reasonably well on language-related tasks like rhyme finding and code generation.\\n- However, it struggles with certain math questions and fails to generate coherent code for complex tasks.\\n\\n**Overall:**\\n\\nMetrol Large 2 shows potential but lacks the consistency and accuracy of established models. Its high price and questionable benchmarks make it less appealing than alternatives like DeepSeek.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "chain = load_summarize_chain(llm=llm, chain_type='refine')\n",
    "\n",
    "chain.run(youtube_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
